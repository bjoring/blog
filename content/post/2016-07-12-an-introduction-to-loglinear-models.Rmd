---
title: An Introduction to Loglinear Models
author: Clay Ford
date: '2016-07-12'
slug: an-introduction-to-loglinear-models
categories:
  - R
tags:
  - loglinear models
---

Loglinear models model cell counts in contingency tables. They're a little different from other modeling methods in that they don't distinguish between response and explanatory variables. All variables in a loglinear model are essentially "responses". 

To learn more about loglinear models, we'll explore the following data from Agresti (1996, Table 6.3). It summarizes responses from a survey that asked high school seniors in a particular city whether they had ever used alcohol, cigarettes, or marijuana. 

![](/img/table_6_3.png)

We're interested in how the cell counts in this table depend on the levels of the categorical variables. If we were interested in modeling, say, marijuana use as a function of alcohol and cigarette use, then we might perform logistic regression. But if we're interested in understanding the relationship between all three variables without one necessarily being the "response", then we might want to try a loglinear model.

First, let's get the data into R. We usually import data from an external file, but since the data is relatively small, we can type it in. 

```{r}
seniors <- array(data = c(911, 44, 538, 456, 
                          3, 2, 43, 279),
                 dim = c(2,2,2),
                 dimnames = list("cigarette" = c("yes","no"),
                                 "marijuana" = c("yes","no"),
                                 "alcohol" = c("yes","no")))
```


We use the <code>array</code> function when we want to create a table with more than two dimensions. The <code>dim</code> argument says we want to create a table with 2 rows, 2 columns, and 2 layers. In other words, we want to create two 2 x 2 tables: cigarette versus marijuana use for each level of alcohol use. The <code>dimnames</code> argument provides names for the dimensions. It requires a list object, so we wrap the argument's input in the <code>list</code> function. The order is rows, columns, then layers. The <code>data</code> argument provides the cell counts. Notice the order of entry. We fill the columns starting at the top left for each layer and work our way down. Finally we assign it to an object called "seniors".

Notice when we print the object, it produces two 2 x 2 tables:

```{r}
seniors

```

If we want to print the table to look like our data source, we need to flatten it. We can do with the <code>ftable</code> function. The <code>row.vars</code> argument specifies which variables we want in the rows, and the order we want them in. Notice we use the names we specified in the <code>dimnames</code> argument when we created the table. 

```{r}
ftable(seniors, row.vars = c("alcohol","cigarette"))

```

Before proceeding to modeling, we may want to explore the data. The <code>addmargins</code> function provides marginal totals. We see that 2276 students participated in the survey and most of them tried alcohol.

```{r}
addmargins(seniors)

```

The <code>prop.table</code> function allows us to calculate cell proportions. It has a <code>margin</code> argument that allows us to specify which margin we want to work across. 1 means rows, 2 means columns, and 3 and above refer to the layers. Below we calculate proportions across the columns along the rows for each layer. Hence the argument <code>margin = c(1,3)</code>:

```{r}
prop.table(seniors, margin = c(1,3))

```

We see that of those students who tried cigarettes and alcohol, 62% also tried marijuana. Likewise, of those students who did not try cigarettes or alcohol, 99% also did not try marijuana. There definitely appears to be some sort of relationship here. 

Now we're ready to try some loglinear modeling. For this article we'll use the <code>glm</code> function, which means we need to transform our data into a different format. While there is a function, <code>loglin</code>, that works directly with contingency tables, we find it easier to work with the formula-based <code>glm</code> function. 

We can easily transform our data by converting it to a data frame. Calling <code>as.data.frame</code> on a table object in R returns a data frame with a column for cell frequencies where each row represents a unique combination of variables. But we also need to convert our array into a table. We can do that with <code>as.table</code>. Below we do it with one line. After that we set the reference level for our three variables to "no". This means the model coefficients will be expressed in terms of "yes" answers, which would seem to be more interesting given the age of our subjects.


```{r}
seniors.df <- as.data.frame(as.table(seniors))
seniors.df[,-4] <- lapply(seniors.df[,-4], 
                          relevel, ref = "no")
seniors.df

```

Now our data is ready for loglinear modeling using the <code>glm</code> function. Before we get started, let's think about why we call it "loglinear". Let's say I flip a quarter 40 times, you flip a nickel 40 times, and we record the results in a 2 x 2 contingency table. Here's a quick simulation:

```{r}
set.seed(1)
qtr <- sample(c("H","T"), 40, replace = T)
nik <- sample(c("H","T"), 40, replace = T)
table(qtr,nik)

```

Since the coins are fair and independent of one another, we'd guess that the cell counts would be roughly equal (about 10 in each cell). In fact, when two categorical variables are independent, we can use their marginal probabilities to calculate their joint probabilities. In our example, the marginal probabilities are 0.5 for the heads and tails of each coin. The joint probabilities are the probabilities of a particular result in the table. For example, the probability of both coins landing heads is 0.5 * 0.5 = 0.25. To get an expected cell count, we multiply that probability by the number of trials. In our case that's 40 * 0.25 = 10. In our simulation, we got 11.

We can generalize this with symbols: \(\mu_{ij} = n\pi_{i}\pi_{j}\). This says the cell count in row i and column j is equal to the product of the marginal probabilities and the number of observations. What we have here is a nice little model that describes how a cell count depends on row and column variables, provided the row and column variables are independent. If we take the log of each side it becomes additive (ie, linear):

$$\log \mu_{ij} = \log n + \log \pi_{i} + \log \pi_{j}$$

Thus we have a "loglinear" model. This particular model is called the loglinear model of independence for two-way contingency tables. If our two variables are not independent, this model does not work well. We would need an additional parameter in our model to allow the two variables to interact. We often call this an interaction term or higher-order term. In loglinear models, those are usually what we're interested in. They allow us to determine if two variables are associated and in what way. 

Let's work with our survey data, which is a three-way contingency table. To start, we model Freq as a function of the three variables using the <code>glm</code> function. Notice we set the <code>family</code> argument to poisson since we're modeling counts. This is known as the independence model. It assumes the three variables are independent of one another, which seems very unlikely. 

```{r}
mod0 <- glm(Freq ~ cigarette + marijuana + alcohol,
            data = seniors.df, family = poisson)
summary(mod0)

```

Looking at the summary it appears this is a great model. We see highly significant coefficients and p-values near 0. But for loglinear models we want to check the residual deviance. As a rule of thumb, we'd like it to be close in value to the degrees of freedom. Here we have 1286 on 4 degrees of freedom. This indicates a poor fit. We can calculate a p-value if we like. The deviance statistic has an approximate chi-squared distribution, so we use the <code>pchisq</code> function. The <code>df.residual</code> function extracts the degrees of freedom. Setting <code>lower.tail = F</code> means we get the probability of seeing a deviance as large or larger than what we observed. 

```{r}
pchisq(deviance(mod0), df = df.residual(mod0), 
       lower.tail = F)

```


The null of this test is that the expected frequencies satisfy the given loglinear model. Clearly they do not!

A more intuitive way to investigate fit is to compare the fitted values to the observed values. One way we can do that is to combine the original data with the fitted values, like so:

```{r}
cbind(mod0$data, fitted(mod0))
```

The fitted model is way off from the observed data. For example, we observed 3 students who tried cigarettes and marijuana but not alcohol. Our model predicts about 90. 

The coefficients in this model can be interpreted as odds if we exponentiate them. For example, if we exponentiate the coefficient for marijuana, we get the following:

```{r}
exp(coef(mod0)[3])

```

That's the odds of using marijuana because "no" is the baseline. We can check this manually by calculating the odds directly:

```{r}
margin.table(seniors, margin = 2)/
  sum(margin.table(seniors, margin = 2))
0.4217926 / 0.5782074
```


According to this model, the odds of using marijuana are about 0.73 to 1, regardless of whether you tried alcohol or cigarettes. However we know that's probably not a good estimate. We can just look at the raw data and see there were many more people who tried marijuana when they also tried cigarettes and alcohol.

Let's fit a more complex model that allows variables to be associated with one another, but maintains the same association regardless of the level of the third variable. We call this homogeneous association. This says that, for example, alcohol and marijuana use have some sort of relationship, but that relationship is the same regardless of whether or not they tried cigarettes. Fitting this model means we add interactions for each pairwise combination of variables. R makes this easy with its formula notation. Simply put parentheses around your variables and add an exponent of 2. This translates to "all variables and all pairwise interactions":

```{r}
mod1 <- glm(Freq ~ (cigarette + marijuana + alcohol)^2, 
            data = seniors.df, family = poisson)
summary(mod1)
```

This model fits much better. Notice the residual deviance (0.37399) compared to the degrees of freedom (1). We can calculate a p-value if we like:

```{r}
pchisq(deviance(mod1), df = df.residual(mod1), 
       lower.tail = F)

```

The high p-value says we have insufficient evidence to reject the null hypothesis that the expected frequencies satisfy our model. Once again we can compare the fitted and observed values and see how well they match up:

```{r}
cbind(mod1$data, fitted(mod1))

```

So the model seems to fit (very well), but how do we describe the association between the variables? What effect do the variables have on one another? To answer this we look at the coefficients of the interactions. By exponentiating the coefficients, we get odds ratios. For example, let's look at the coefficient for "marijuanayes:alcoholyes".

```{r}
exp(coef(mod1)["marijuanayes:alcoholyes"])

```

Students who tried marijuana have estimated odds of having tried alcohol that are 19 times the estimated odds for students who did not try marijuana. Because we fit a homogeneous association model, the odds ratio is the same regardless of whether they tried cigarettes. 

It's a good idea to calculate a confidence interval for these odds ratio estimates. We can do that with <code>confint</code> function:

```{r}
exp(confint(mod1, parm = c("cigaretteyes:marijuanayes",
                           "cigaretteyes:alcoholyes",
                           "marijuanayes:alcoholyes")))
```

The associations are all pretty strong. We see that the odds of trying marijuana if you tried cigarettes is at least 12 times higher than the odds of trying marijuana if you hadn't tried cigarettes, and vice versa. 

What happens if we fit a model with a three-way interaction? This allows the pairwise associations to change according the level of the third variable. This is the saturated model since it has as many coefficients as cells in our table: 8. It perfectly fits the data. The formula "cigarette * marijuana * alcohol" means fit all interactions. 

```{r}
mod2 <- glm(Freq ~ cigarette * marijuana * alcohol, 
            data = seniors.df, family = poisson) 

```

The deviance of this model is basically 0 on 0 degrees of freedom. The fitted counts match the observed counts:

```{r}
deviance(mod2)
cbind(mod2$data, fitted(mod2))

```


All things being equal, we prefer a simpler model. We usually don't want to finish with a saturated model that perfectly fits our data. However it's useful to fit a saturated model to verify the higher-order interaction is statistically not significant. We can verify that the homogeneous association model fits just as well as the saturated model by performing a likelihood ratio test. One way to do this is with the <code>anova</code> function:

```{r}
anova(mod1, mod2)

```

This says we fail to reject the null hypothesis that mod1 fits just as well as mod2.

We could try models with only certain interactions. For example, look at the following model:

```{r}
mod3 <- glm(Freq ~ (cigarette * marijuana) + 
              (alcohol * marijuana),
            data = seniors.df, family = poisson) 

```

This fits interactions for cigarette and marijuana, and alcohol and marijuana, but not cigarette and alcohol. The implication is that alcohol and cigarette use is independent of one another, controlling for marijuana use. Is this a suitable model? Once again we can perform a likelihood ratio test:

```{r}
anova(mod3, mod1)
pchisq(187.38, df = 1, lower.tail = F)
```


The p-value is tiny. The probability of seeing such a big change in deviance (187.38) if the models really were no different is remote. There appears to be good evidence that the homogeneous association model provides a much better fit than the model that assumes conditional independence between alcohol and cigarette use. 

Loglinear models work for larger tables that extend into 4 or more dimensions. Obviously the interpretation of interactions becomes much more complicated when they involve 3 or more variables. 

To learn more about loglinear models, see the references below. The example for this blog post comes from Chapter 6 of An Introduction to Categorical Data Analysis. 

### References:

- Agresti, A. <em>An Introduction to Categorical Data Analysis</em>, 1st Ed. 1996. Ch. 6.
- Faraway, J. <em>Extending the Linear Model with R</em>. 2006. Ch. 4.
- Venables, W.N and Ripley, B.D. <em>Modern Applied Statistics with S</em>, 4th Ed. 2002. Ch. 7.

For questions or clarifications regarding this article, contact the UVa Library StatLab: [statlab@virginia.edu](mailto:statlab@virginia.edu) 

_Clay Ford_   
_Statistical Research Consultant_  
_University of Virginia Library_  

```{r}
sessionInfo()
```
