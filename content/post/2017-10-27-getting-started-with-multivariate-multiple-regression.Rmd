---
title: Getting started with Multivariate Multiple Regression
author: Clay Ford
date: '2017-10-27'
slug: getting-started-with-multivariate-multiple-regression
categories:
  - R
tags:
  - multivariate multiple regression
  - manova
---

Multivariate Multiple Regression is the method of modeling multiple responses, or dependent variables, with a single set of predictor variables. For example, we might want to model both math and reading SAT scores as a function of gender, race, parent income, and so forth. This allows us to evaluate the relationship of, say, gender with each score. You may be thinking, "why not just run separate regressions for each dependent variable?" That's actually a good idea! And in fact that's pretty much what multivariate multiple regression does. It regresses each dependent variable separately on the predictors. However, because we have multiple responses, we have to modify our hypothesis tests for regression parameters and our confidence intervals for predictions.

To get started, let's read in some data from the book <em>Applied Multivariate Statistical Analysis (6th ed.)</em> by Richard Johnson and Dean Wichern. This data come from exercise 7.25 and involve 17 overdoses of the drug amitriptyline (Rudorfer, 1982). There are two responses we want to model: TOT and AMI. TOT is total TCAD plasma level and AMI is the amount of amitriptyline present in the TCAD plasma level. The predictors are as follows:

- GEN, gender (male = 0, female = 1)
- AMT, amount of drug taken at time of overdose
- PR, <a href="https://en.wikipedia.org/wiki/PR_interval" target="_blank">PR wave measurement</a>
- DIAP, diastolic blood pressure
- QRS, <a href="https://en.wikipedia.org/wiki/QRS_complex" target="_blank">QRS wave measurement</a>

We'll use the <a href="https://www.r-project.org/" target="_blank">R statistical computing environment</a> to demonstrate multivariate multiple regression. The following code reads the data into R and names the columns.

```{r}
ami_data <- read.table("http://static.lib.virginia.edu/statlab/materials/data/ami_data.DAT")
names(ami_data) <- c("TOT","AMI","GEN","AMT","PR","DIAP","QRS")

```

Before going further you may wish to explore the data using the summary and pairs functions.

```{r eval=FALSE}
summary(ami_data)
pairs(ami_data)
```

Performing multivariate multiple regression in R requires wrapping the multiple responses in the `cbind` function. `cbind` takes two vectors, or columns, and "binds" them together into two columns of data. We insert that on the left side of the formula operator: `~`. On the other side we add our predictors. The `+` signs do not mean addition per se but rather inclusion. Taken together the formula `cbind(TOT, AMI) ~ GEN + AMT + PR + DIAP + QRS` translates to "model TOT and AMI as a function of GEN, AMT, PR, DIAP and QRS." To fit this model we use the workhorse `lm` function and save it to an object we named "mlm1". Finally we view the results with `summary`.

```{r}
mlm1 <- lm(cbind(TOT, AMI) ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(mlm1)
```

Notice the summary shows the results of two regressions: one for TOT and one for AMI. These are exactly the same results we would get if modeled each separately. You can verify this for yourself by running the following code and comparing the summaries to what we got above. They're identical.

```{r eval=FALSE}
m1 <- lm(TOT ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(m1)
m2 <- lm(AMI ~ GEN + AMT + PR + DIAP + QRS, data = ami_data)
summary(m2)

```


The same diagnostics we check for models with one predictor should be checked for these as well. For a review of some basic but essential diagnostics see our post <a href="http://data.library.virginia.edu/diagnostic-plots/" target="_blank">Understanding Diagnostic Plots for Linear Regression Analysis</a>. 

We can use R's extractor functions with our mlm1 object, except we'll get double the output. For example, instead of one set of residuals, we get two:

```{r}
head(resid(mlm1))
```

Instead of one set of fitted values, we get two:

```{r}
head(fitted(mlm1))

```


Instead of one set of coefficients, we get two:


```{r}
coef(mlm1)
```

Again these are all identical to what we get by running separate models for each response. The similarity ends, however, with the variance-covariance matrix of the model coefficients. We don't reproduce the output here because of the size, but we encourage you to view it for yourself:

```{r eval=FALSE}
vcov(mlm1)

```

The main takeaway is that the coefficients from both models covary. That covariance needs to be taken into account when determining if a predictor is jointly contributing to both models. For example, the effects of PR and DIAP seem borderline. They appear significant for TOT but less so for AMI. But it's not enough to eyeball the results from the two separate regressions! We need to formally test for their inclusion. And that test involves the covariances between the coefficients in both models. 

Determining whether or not to include predictors in a multivariate multiple regression requires the use of multivariate test statistics. These are often taught in the context of MANOVA, or multivariate analysis of variance. Again the term "multivariate" here refers to multiple responses or dependent variables. This means we use modified hypothesis tests to determine whether a predictor contributes to a model. 

The easiest way to do this is to use the `Anova` or `Manova` functions in the `car` package (Fox and Weisberg, 2011), like so:

```{r message=FALSE}
library(car)
Anova(mlm1)
```

The results are titled "Type II MANOVA Tests". The `Anova` function automatically detects that mlm1 is a multivariate multiple regression object. "Type II" refers to the type of sum-of-squares. This basically says that predictors are tested assuming all other predictors are already in the model. This is usually what we want. Notice that PR and DIAP appear to be jointly insignificant for the two models despite what we were led to believe by examining each model separately. 

Based on these results we may want to see if a model with just GEN and AMT fits as well as a model with all five predictors. One way we can do this is to fit a smaller model and then compare the smaller model to the larger model using the `anova` function, (notice the little "a"; this is different from the `Anova` function in the `car` package). For example, below we create a new model using the `update` function that only includes GEN and AMT. The expression `. ~ . - PR - DIAP - QRS` says "keep the same responses and predictors except PR, DIAP and QRS."


```{r}
mlm2 <- update(mlm1, . ~ . - PR - DIAP - QRS)
anova(mlm1, mlm2)
```

The large p-value provides good evidence that the model with two predictors fits as well as the model with five predictors. Notice the test statistic is "Pillai", which is one of the four common multivariate test statistics. 

The `car` package provides another way to conduct the same test using the `linearHypothesis` function. The beauty of this function is that it allows us to run the test without fitting a separate model. It also returns all four multivariate test statistics. The first argument to the function is our model. The second argument is our null hypothesis. The `linearHypothesis` function conveniently allows us to enter this hypothesis as character phrases. The null entered below is that the coefficients for PR, DIAP and QRS are all 0.

```{r}
lh.out <- linearHypothesis(mlm1, hypothesis.matrix = c("PR = 0", "DIAP = 0", "QRS = 0"))
lh.out

```

The Pillai result is the same as we got using the `anova` function above. The Wilks, Hotelling-Lawley, and Roy results are different versions of the same test. The consensus is that the coefficients for PR, DIAP and QRS do not seem to be statistically different from 0. There is some discrepancy in the test results. The Roy test in particular is significant, but this is likely due to the small sample size (n = 17). 

Also included in the output are two sum of squares and products matrices, one for the hypothesis and the other for the error. These matrices are used to calculate the four test statistics. These matrices are stored in the lh.out object as SSPH (hypothesis) and SSPE (error). We can use these to manually calculate the test statistics. For example, let SSPH = H and SSPE = E. The formula for the Wilks test statistic is 

$$
\frac{\begin{vmatrix}\bf{E}\end{vmatrix}}{\begin{vmatrix}\bf{E} + \bf{H}\end{vmatrix}}
$$

In R we can calculate that as follows:

```{r}
E <- lh.out$SSPE
H <- lh.out$SSPH
det(E)/det(E + H)

```


Likewise the formula for Pillai is

$$
tr[\bf{H}(\bf{H} + \bf{E})^{-1}]
$$

tr means trace. That's the sum of the diagonal elements of a matrix. In R we can calculate as follows:


```{r}
sum(diag(H %*% solve(E + H)))

```


The formula for Hotelling-Lawley is 

$$
tr[\bf{H}\bf{E}^{-1}]
$$

In R:

```{r}
sum(diag(H %*% solve(E)))

```


And finally the Roy statistic is the largest eigenvalue of $\bf{H}\bf{E}^{-1}$.

In R code:

```{r}
e.out <- eigen(H %*% solve(E))
max(e.out$values)
```

Given these test results, we may decide to drop PR, DIAP and QRS from our model. In fact this is model mlm2 that we fit above. Here is the summary:

```{r}
summary(mlm2)

```


Now let's say we wanted to use this model to predict TOT and AMI for GEN = 1 (female) and AMT = 1200. We can use the `predict` function for this. First we need put our new data into a data frame with column names that match our original data.

```{r}
nd <- data.frame(GEN = 1, AMT = 1200)
p<- predict(mlm2, nd)
p

```

This predicts two values, one for each response. Now this is just a prediction and has uncertainty. We usually quantify uncertainty with confidence intervals to give us some idea of a lower and upper bound on our estimate. But in this case we have two predictions from a multivariate model with two sets of coefficients that covary! This means calculating a confidence interval is more difficult. In fact we don't calculate an interval but rather an ellipse to capture the uncertainty in two dimensions.

Unfortunately at the time of this writing there doesn't appear to be a function in R for creating uncertainty ellipses for multivariate multiple regression models with two responses. However we have written one below you can use called `predictionEllipse`. The details of the function go beyond a "getting started" blog post but it should be easy enough to use. Simply submit the code in the console to create the function. Then use the function with any multivariate multiple regression model object that has two responses. The newdata argument works the same as the newdata argument for predict. Use the level argument to specify a confidence level between 0 and 1. The default is 0.95. Set ggplot to FALSE to create the plot using base R graphics. 

```{r}
predictionEllipse <- function(mod, newdata, level = 0.95, ggplot = TRUE){
  # labels
  lev_lbl <- paste0(level * 100, "%")
  resps <- colnames(mod$coefficients)
  title <- paste(lev_lbl, "confidence ellipse for", resps[1], "and", resps[2])
  
  # prediction
  p <- predict(mod, newdata)
  
  # center of ellipse
  cent <- c(p[1,1],p[1,2])
  
  # shape of ellipse
  Z <- model.matrix(mod)
  Y <- mod$model[[1]]
  n <- nrow(Y)
  m <- ncol(Y)
  r <- ncol(Z) - 1
  S <- crossprod(resid(mod))/(n-r-1)
  
  # radius of circle generating the ellipse
  tt <- terms(mod)
  Terms <- delete.response(tt)
  mf <- model.frame(Terms, newdata, na.action = na.pass, 
                   xlev = mod$xlevels)
  z0 <- model.matrix(Terms, mf, contrasts.arg = mod$contrasts)
  rad <- sqrt((m*(n-r-1)/(n-r-m))*qf(level,m,n-r-m)*z0%*%solve(t(Z)%*%Z) %*% t(z0))
  
  # generate ellipse using ellipse function in car package
  ell_points <- car::ellipse(center = c(cent), shape = S, radius = c(rad), draw = FALSE)
  
  # ggplot2 plot
  if(ggplot){
    require(ggplot2, quietly = TRUE)
    ell_points_df <- as.data.frame(ell_points)
    ggplot(ell_points_df, aes(x, y)) +
      geom_path() +
      geom_point(aes(x = TOT, y = AMI), data = data.frame(p)) +
      labs(x = resps[1], y = resps[2], 
           title = title)
  } else {
    # base R plot
    plot(ell_points, type = "l", xlab = resps[1], ylab = resps[2], main = title)
    points(x = cent[1], y = cent[2])
  }
}
```

Here's a demonstration of the function.

```{r}
predictionEllipse(mod = mlm2, newdata = nd)

```

The dot in the center is our predicted values for TOT and AMI. The ellipse represents the uncertainty in this prediction. We're 95% confident the true values of TOT and AMI when GEN = 1 and AMT = 1200 are within the area of the ellipse. Notice also that TOT and AMI seem to be positively correlated. Predicting higher values of TOT means predicting higher values of AMI, and vice versa.

### References

- Fox, J and Weisberg, S (2011). <em>An {R} Companion to Applied Regression, Second Edition</em>. Thousand Oaks CA: Sage. URL: <a href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion" target="_blank">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</a>
- Johnson, R and Wichern, D (2007). <em>Applied Multivariate Statistical Analysis, Sixth Edition</em>. Prentice-Hall.
- Rudorfer, MV "Cardiovascular Changes and Plasma Drug Levels after Amitriptyline Overdose." <em>Journal of Toxicology-Clinical Toxicology</em>, 19 (1982), 67-71.


For questions or clarifications regarding this article, contact the UVa Library StatLab: [statlab@virginia.edu](mailto:statlab@virginia.edu) 

_Clay Ford_  
_Statistical Research Consultant_  
_University of Virginia Library_   

```{r}
sessionInfo()
```

